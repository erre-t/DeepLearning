import numpy as np
from matplotlib import pyplot as plt


class Neural_network():
    def __init__(self, load_all_data, shapes, activations, GD_params):
        self.load_data(load_all_data)
        self.init_parameters(shapes)
        self.activations = activations
        self.set_GD_params(GD_params)

    def set_GD_params(self, GD_params):
        cycles = GD_params["cycles"]
        n_s = GD_params["n_s"]
        batch_size = GD_params["batch_size"]
        N = len(self.X_train[1])
        epochs = (n_s * 2) * cycles / (N / batch_size)

        if (epochs % 2 != 0):
            raise Exception("Epochs is not a whole number")

        GD_params["epochs"] = int(epochs)

        self.GD_params = GD_params

    def load_data(self, load_all_data):
        if load_all_data:
            self.X_train, self.Y_train, self.X_valid, self.Y_valid, self.X_test, self.Y_test = getAllData()
        else:
            self.X_train, self.Y_train, self.X_valid, self.Y_valid, self.X_test, self.Y_test = \
                getData("data_batch_1", "data_batch_2", "test_batch")

    def init_parameters(self, shapes):
        self.d = np.shape(self.X_train)[0]  # 3072 pixels
        self.K = np.shape(self.Y_train)[0]  # number of classes / 10.
        self.m = 50  # Nodes in hidden layer
        self.W = []
        self.B = []
        self.layers = len(shapes)

        for shape in shapes:
            w = np.random.normal(0.0, 1 / np.sqrt(shape[1]), size=shape)
            b = np.zeros(shape[0]).reshape(shape[0], 1)

            self.W.append(w)
            self.B.append(b)

    def mini_batch_GD(self):
        # Unpack
        batch_size = self.GD_params["batch_size"]
        epochs = self.GD_params["epochs"]

        x_batches, y_batches = generate_mini_batches(self.X_train, self.Y_train, batch_size)
        costs_train = []
        costs_valid = []
        losses_train = []
        losses_valid = []
        accuracies_train = []
        accuracies_valid = []
        etas = []

        for i in range(epochs):
            for j in range(len(x_batches)):
                eta = self.update_eta(get_t(i, j + 1, len(x_batches)))
                etas.append(eta)
                W_grad, B_grad = self.computeGrads(x_batches[j], y_batches[j])

                for layer in range(self.layers):
                    self.W[layer] -= eta * W_grad[layer]
                    self.B[layer] -= eta * B_grad[layer]

            #cost, loss = self.compute_cost(self.X_train, self.Y_train)
            accuracy_train = self.compute_accuracy(self.X_train, self.Y_train)
            accuracy_valid = self.compute_accuracy(self.X_valid, self.Y_valid)
            accuracies_train.append(accuracy_train)
            accuracies_valid.append(accuracy_valid)
        plot_acc(accuracies_train, accuracies_valid, len(x_batches))
        print(accuracy_valid)

    def compute_cost(self, X, Y):
        lamda = self.GD_params["lamda"]
        P, H = self.evaluateClassifier(X)
        N = X.shape[1]
        l = np.sum(lossFunction(Y, P)) / N
        for w in self.W:
            r = np.sum(w ** 2)

        J = l + lamda * r

        return J, l

    def computeGrads(self, X_batch, Y_batch):
        # Unpack
        batch_size = self.GD_params["batch_size"]
        lamda = self.GD_params["lamda"]

        # init grads
        W_grad = np.zeros_like(self.W)
        B_grad = np.zeros_like(self.B)

        # Forward
        P_batch, H_batch = self.evaluateClassifier(X_batch)

        # Backward
        G_batch = - (Y_batch - P_batch)

        for layer in range(self.layers - 1, 0, -1):
            W_grad[layer] = ((1 / batch_size) * np.dot(G_batch, np.transpose(H_batch[layer - 1]))) + 2 * lamda * self.W[
                layer]
            B_grad[layer] = (1 / batch_size) * np.sum(G_batch, 1)
            B_grad[layer] = B_grad[layer].reshape(-1, 1)

            G_batch = np.dot(np.transpose(self.W[layer]), G_batch)
            G_batch = np.multiply(G_batch, H_batch[layer - 1] > 0)

        W_grad[0] = (1 / batch_size) * np.dot(G_batch, np.transpose(X_batch)) + lamda * self.W[0]  # TODO: 2*lamda?
        B_grad[0] = (1 / batch_size) * np.dot(G_batch, np.ones(batch_size))
        B_grad[0] = B_grad[0].reshape(-1, 1)

        return [W_grad, B_grad]

    def evaluateClassifier(self, x_batch):

        H = []
        S = []
        s = x_batch.copy()  # s=x in first iteration

        for w, b, activation in zip(self.W, self.B, self.activations):
            s = np.dot(w, s) + b
            if activation == "relu":
                s = np.maximum(0, s)
                H.append(s)
            if activation == "softmax":
                P = soft_max(s)
            S.append(s)

        return P, H

    def compute_gradients_num(self, X_batch, Y_batch, size, h):

        grad_W = []
        grad_B = []
        for j in range(len(self.B)):
            grad_W.append(np.zeros(self.W[j].shape))
            for i in range(len(self.W[j].flatten())):
                old_par = self.W[j].flat[i]
                self.W[j].flat[i] = old_par + h
                _, c2 = self.compute_cost(X_batch, Y_batch)
                self.W[j].flat[i] = old_par - h
                _, c3 = self.compute_cost(X_batch, Y_batch)
                self.W[j].flat[i] = old_par
                grad_W[j].flat[i] = (c2 - c3) / (2 * h)

        for j in range(len(self.B)):
            grad_B.append(np.zeros(self.B[j].shape))
            for i in range(len(self.B[j].flatten())):
                old_par = self.B[j].flat[i]
                self.B[j].flat[i] = old_par + h
                _, c2 = self.compute_cost(X_batch, Y_batch)
                self.B[j].flat[i] = old_par - h
                _, c3 = self.compute_cost(X_batch, Y_batch)
                self.B[j].flat[i] = old_par
                grad_B[j].flat[i] = (c2 - c3) / (2 * h)

        return grad_W, grad_B

    def compare_gradient(self):
        batch_size = self.GD_params["batch_size"]
        eps = 1e-6

        W_num, B_num = self.compute_gradients_num(self.X_train[:100, 0:batch_size], self.Y_train[:100, 0:batch_size], 2,
                                                  eps)
        W_anal, B_anal = self.computeGrads(self.X_train[:100, 0:batch_size], self.Y_train[:100, 0:batch_size])
        print("Jens")

        for layer in range(self.layers):
            W_diff = np.sum(abs(W_anal[layer] - W_num[layer])) / max(eps, (
                    np.sum(abs(W_anal[layer])) + np.sum(abs(W_num[layer]))))
            b_diff = np.sum(abs(B_anal[layer] - B_num[layer])) / max(eps, (
                    np.sum(abs(B_anal[layer])) + np.sum(abs(B_num[layer]))))
            print("W num sum in layer: " + str(layer) + ": = " + str(np.sum(W_num[layer])))
            print("W anal sum in layer" + str(layer) + ": = " + str(np.sum(W_anal[layer])))
            print("b num sum in layer" + str(layer) + ": = " + str(np.sum(B_num[layer])))
            print("b anal sum in layer " + str(layer) + ": = " + str(np.sum(B_anal[layer])))
            print("Difference in W in layer " + str(layer) + ": = " + str(W_diff) + " with batch size: " + str(
                batch_size))
            print("Difference in B in layer: " + str(layer) + ": = " + str(b_diff) + " with batch size: " + str(
                batch_size))

    def update_eta(self, t):
        eta_min = self.GD_params["eta_min"]
        eta_max = self.GD_params["eta_max"]
        n_s = self.GD_params["n_s"]
        l = (t // (n_s * 2))

        if (2 * l * n_s) <= t and t <= ((2 * l + 1) * n_s):
            eta = eta_min + ((t - 2 * l * n_s) * (eta_max - eta_min)) / n_s
        else:
            eta = eta_max - (((t - (2 * l + 1) * n_s) / n_s) * (eta_max - eta_min))

        return eta

    def compute_accuracy(self, X, Y):
        P, H = self.evaluateClassifier(X)

        Y_pred = np.argmax(P, 0)
        Y = np.argmax(Y, 0)

        number_of_correct = np.sum(Y_pred == Y)

        N = X.shape[1]
        acc = number_of_correct / N

        return acc


def get_t(i, j, batch_size):
    return i * batch_size + j


def soft_max(s):
    """ Standard definition of the softmax function """
    return np.exp(s) / np.sum(np.exp(s), axis=0)


def LoadBatch(filename):
    """ Copied from the dataset website """
    import pickle
    with open('./Dataset/' + filename, 'rb') as fo:
        dict = pickle.load(fo, encoding='bytes')
        dict[b'data'] = dict[b'data'].astype(float)

    return dict


def getData(train_file, valid_file, test_file):
    # Load data
    train_data = LoadBatch(train_file)
    valid_data = LoadBatch(valid_file)
    test_data = LoadBatch(test_file)

    # Extract data
    X_train = train_data.get(b'data')
    X_valid = valid_data.get(b'data')
    X_test = test_data.get(b'data')
    Y_train = train_data.get(b'labels')
    Y_valid = valid_data.get(b'labels')
    Y_test = test_data.get(b'labels')

    # Normalize
    X_train, X_valid, X_test = \
        normalize(X_train, X_valid, X_test)

    # One hot encoding of Y
    K = len(np.unique(train_data[b'labels']))  # Number of labels, used to create one_hot_matrix
    Y_train = one_hot_matrix(Y_train, K)
    Y_valid = one_hot_matrix(Y_valid, K)
    Y_test = one_hot_matrix(Y_test, K)

    # Transpose X
    X_train = np.transpose(X_train)
    X_valid = np.transpose(X_valid)
    X_test = np.transpose(X_test)

    return X_train, Y_train, X_valid, Y_valid, X_test, Y_test


def getAllData():
    # Training
    batch_1 = LoadBatch("data_batch_1")
    X_1 = batch_1.get(b'data')
    Y_1 = batch_1.get(b'labels')

    batch_2 = LoadBatch("data_batch_2")
    X_2 = batch_2.get(b'data')
    Y_2 = batch_2.get(b'labels')

    batch_3 = LoadBatch("data_batch_3")
    X_3 = batch_3.get(b'data')
    Y_3 = batch_3.get(b'labels')

    batch_4 = LoadBatch("data_batch_4")
    X_4 = batch_4.get(b'data')
    Y_4 = batch_4.get(b'labels')

    batch_5 = LoadBatch("data_batch_5")
    X_5 = batch_5.get(b'data')
    Y_5 = batch_5.get(b'labels')

    # Test
    test_batch = LoadBatch("test_batch")
    X_test = test_batch.get(b'data')
    Y_test = test_batch.get(b'labels')

    # One hot encoding
    K = len(np.unique(Y_1))  # Number of labels, used to create one_hot_matrix
    Y_1 = one_hot_matrix(Y_1, K)
    Y_2 = one_hot_matrix(Y_2, K)
    Y_3 = one_hot_matrix(Y_3, K)
    Y_4 = one_hot_matrix(Y_4, K)
    Y_5 = one_hot_matrix(Y_5, K)
    Y_test = one_hot_matrix(Y_test, K)

    # Concatenate
    X_train = np.concatenate(
        (X_1[0:9000, :], X_2[0:9000, :], X_3[0:9000, :], X_4[0:9000, :], X_5[0:9000, :]), axis=0)

    X_valid = np.concatenate((X_1[9000:10000, :], X_2[9000:10000, :], X_3[9000:10000, :],
                              X_4[9000:10000, :], X_5[9000:10000, :]), axis=0)

    Y_train = np.concatenate((Y_1[:, 0:9000], Y_2[:, 0:9000], Y_3[:, 0:9000], Y_4[:, 0:9000], Y_5[:, 0:9000]), axis=1)

    Y_valid = np.concatenate((Y_1[:, 9000:10000], Y_2[:, 9000:10000], Y_3[:, 9000:10000],
                              Y_4[:, 9000:10000], Y_5[:, 9000:10000]), axis=1)

    # Normalize
    X_train, X_valid, X_test = normalize(X_train, X_valid, X_test)

    # Transpose X
    X_train = np.transpose(X_train)
    X_valid = np.transpose(X_valid)
    X_test = np.transpose(X_test)

    return X_train, Y_train, X_valid, Y_valid, X_test, Y_test


def lossFunction(Y, P):
    l = - Y * np.log(P)

    return l


def normalize(train_data, valid_data, test_data):
    mean = np.mean(train_data, 0)
    std = np.std(train_data, 0)

    train_data = train_data - mean
    train_data = train_data / std

    valid_data = valid_data - mean
    valid_data = valid_data / std

    test_data = test_data - mean
    test_data = test_data / std

    return train_data, valid_data, test_data


def generate_mini_batches(X, Y, batch_size):
    # TODO: solve non-even division
    n_batches = int(X.shape[1] / batch_size)

    x_batches = np.zeros((n_batches, X.shape[0], batch_size))
    y_batches = np.zeros((n_batches, Y.shape[0], batch_size))

    for i in range(n_batches):
        x_batches[i] = X[:, i * batch_size: (i + 1) * batch_size]
        y_batches[i] = Y[:, i * batch_size: (i + 1) * batch_size]

    return x_batches, y_batches


def one_hot_matrix(Y, K):
    Y_hot = np.zeros((len(Y), K))
    Y_hot[np.arange(len(Y)), Y] = 1
    Y_hot = np.transpose(Y_hot)

    return Y_hot


def plot_costs(costs_train, costs_valid):
    x = range(0, len(costs_train) * 100, 1)
    plt.plot(x, costs_train, label="Training cost")
    plt.plot(x, costs_valid, label="Validation cost")
    plt.xlabel("Updates")
    plt.ylabel("Cost")
    plt.title("Graph comparing training and validation cost")
    plt.legend()
    plt.show()


def plot_costs(costs_train, costs_valid, batch_size):
    x = range(0, len(costs_train) * batch_size, batch_size)
    plt.plot(x, costs_train, label="Training cost")
    plt.plot(x, costs_valid, label="Validation cost")
    plt.xlabel("Updated")
    plt.ylabel("Cost")
    plt.title("Graph comparing training and validation cost")
    plt.legend()
    plt.show()


def plot_loss(loss_train, loss_valid, batch_size):
    x = range(0, len(loss_train) * batch_size, batch_size)
    plt.plot(x, loss_train, label="Training loss")
    plt.plot(x, loss_valid, label="Validation loss")
    plt.xlabel("Updates")
    plt.ylabel("Loss")
    plt.title("Graph comparing training and validation loss")
    plt.legend()
    plt.show()


def  plot_acc(acc_train, acc_valid, batch_size):
    x = range(0, len(acc_train) * batch_size, batch_size)
    plt.plot(x, acc_train, label="Training accuracy")
    plt.plot(x, acc_valid, label="Validation accuracy")
    plt.xlabel("Updates")
    plt.ylabel("Accuracy")
    plt.title("Graph comparing training and validation accuracy")
    plt.legend()
    plt.show()


def main():
    # Set seed of random generator
    np.random.seed(seed=400)

    # Parameters for network architecture and training
    GD_params = {"batch_size": 100, "eta_min": 1e-5, "eta_max": 0.1, "n_s": 2250,
                 "cycles": 2, "lamda": 0}
    # shapes = [(50, 3072), (50, 50), (10, 50)]
    # activations = ["relu", "relu", "softmax"]
    shapes = [(50, 3072), (50, 50), (10, 50)]
    activations = ["relu", "relu", "softmax"]

    # Create network
    neural_network = Neural_network(load_all_data=True, shapes=shapes, activations=activations, GD_params=GD_params)
    #neural_network.mini_batch_GD()
    neural_network.compare_gradient()

    # acc = ComputeAccuracy(X_test, Y_test, W1, W2, b1, b2)


main()
